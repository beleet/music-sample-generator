{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training MusicLM model\n",
    "### Robert Chen, Ahmadsho Akdodshoev, Philip Timofeev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mutable default <class 'fairseq.dataclass.configs.CommonConfig'> for field common is not allowed: use default_factory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchaudio\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmusiclm_pytorch\u001b[39;00m \u001b[39mimport\u001b[39;00m MuLaN, MuLaNEmbedQuantizer, MuLaNTrainer, \\\n\u001b[1;32m      5\u001b[0m                             AudioSpectrogramTransformer, TextTransformer, MusicLM\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maudiolm_pytorch\u001b[39;00m \u001b[39mimport\u001b[39;00m SemanticTransformer, SemanticTransformerTrainer, \\\n\u001b[1;32m      7\u001b[0m                             CoarseTransformer, CoarseTransformerTrainer, \\\n\u001b[1;32m      8\u001b[0m                             FineTransformer, FineTransformerTrainer, \\\n\u001b[1;32m      9\u001b[0m                             AudioLM, HubertWithKmeans, MusicLMSoundStream, \\\n\u001b[1;32m     10\u001b[0m                             SoundStreamTrainer, SoundStream \n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/musiclm_pytorch/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmusiclm_pytorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmusiclm_pytorch\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     MuLaN,\n\u001b[1;32m      3\u001b[0m     MuLaNEmbedQuantizer,\n\u001b[1;32m      4\u001b[0m     MusicLM,\n\u001b[1;32m      5\u001b[0m     AudioSpectrogramTransformer,\n\u001b[1;32m      6\u001b[0m     TextTransformer,\n\u001b[1;32m      7\u001b[0m     SigmoidContrastiveLearning,\n\u001b[1;32m      8\u001b[0m     SoftmaxContrastiveLearning\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmusiclm_pytorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m \u001b[39mimport\u001b[39;00m MuLaNTrainer\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/musiclm_pytorch/musiclm_pytorch.py:10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn, einsum\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchaudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m Spectrogram, TimeStretch, FrequencyMasking, TimeMasking\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maudiolm_pytorch\u001b[39;00m \u001b[39mimport\u001b[39;00m AudioLM\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maudiolm_pytorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m AudioConditionerBase\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdist\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/audiolm_pytorch/__init__.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39meinops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_torch_specific\u001b[39;00m \u001b[39mimport\u001b[39;00m allow_ops_in_compiled_graph\n\u001b[1;32m      6\u001b[0m     allow_ops_in_compiled_graph()\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maudiolm_pytorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudiolm_pytorch\u001b[39;00m \u001b[39mimport\u001b[39;00m AudioLM\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maudiolm_pytorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msoundstream\u001b[39;00m \u001b[39mimport\u001b[39;00m SoundStream, AudioLMSoundStream, MusicLMSoundStream\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maudiolm_pytorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mencodec\u001b[39;00m \u001b[39mimport\u001b[39;00m EncodecWrapper\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/audiolm_pytorch/audiolm_pytorch.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39meinops\u001b[39;00m \u001b[39mimport\u001b[39;00m rearrange, repeat, reduce\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39meinops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Rearrange\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maudiolm_pytorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvq_wav2vec\u001b[39;00m \u001b[39mimport\u001b[39;00m FairseqVQWav2Vec\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maudiolm_pytorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhubert_kmeans\u001b[39;00m \u001b[39mimport\u001b[39;00m HubertWithKmeans\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maudiolm_pytorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mt5\u001b[39;00m \u001b[39mimport\u001b[39;00m t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/audiolm_pytorch/vq_wav2vec.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39meinops\u001b[39;00m \u001b[39mimport\u001b[39;00m rearrange\n\u001b[0;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfairseq\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchaudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m resample\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maudiolm_pytorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m curtail_to_multiple\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/fairseq/__init__.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mpdb\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[39m# backwards compatibility to support `from fairseq.X import Y`\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfairseq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m \u001b[39mimport\u001b[39;00m utils \u001b[39mas\u001b[39;00m distributed_utils\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfairseq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mimport\u001b[39;00m meters, metrics, progress_bar  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m     23\u001b[0m sys\u001b[39m.\u001b[39mmodules[\u001b[39m\"\u001b[39m\u001b[39mfairseq.distributed_utils\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m distributed_utils\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/fairseq/distributed/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributed_timeout_wrapper\u001b[39;00m \u001b[39mimport\u001b[39;00m DistributedTimeoutWrapper\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfully_sharded_data_parallel\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     fsdp_enable_wrap,\n\u001b[1;32m      9\u001b[0m     fsdp_wrap,\n\u001b[1;32m     10\u001b[0m     FullyShardedDataParallel,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlegacy_distributed_data_parallel\u001b[39;00m \u001b[39mimport\u001b[39;00m LegacyDistributedDataParallel\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodule_proxy_wrapper\u001b[39;00m \u001b[39mimport\u001b[39;00m ModuleProxyWrapper\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/fairseq/distributed/fully_sharded_data_parallel.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfairseq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataclass\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfigs\u001b[39;00m \u001b[39mimport\u001b[39;00m DistributedTrainingConfig\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfairseq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m \u001b[39mimport\u001b[39;00m utils \u001b[39mas\u001b[39;00m dist_utils\n\u001b[1;32m     14\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/fairseq/dataclass/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconfigs\u001b[39;00m \u001b[39mimport\u001b[39;00m FairseqDataclass\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconstants\u001b[39;00m \u001b[39mimport\u001b[39;00m ChoiceEnum\n\u001b[1;32m     10\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFairseqDataclass\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mChoiceEnum\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/fairseq/dataclass/configs.py:1104\u001b[0m\n\u001b[1;32m   1095\u001b[0m     ema_update_freq: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m field(\n\u001b[1;32m   1096\u001b[0m         default\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, metadata\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mhelp\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDo EMA update every this many model updates\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m   1097\u001b[0m     )\n\u001b[1;32m   1098\u001b[0m     ema_fp32: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m field(\n\u001b[1;32m   1099\u001b[0m         default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1100\u001b[0m         metadata\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mhelp\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mIf true, store EMA model in fp32 even if model is in fp16\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m   1101\u001b[0m     )\n\u001b[0;32m-> 1104\u001b[0m \u001b[39m@dataclass\u001b[39m\n\u001b[1;32m   1105\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mFairseqConfig\u001b[39;00m(FairseqDataclass):\n\u001b[1;32m   1106\u001b[0m     common: CommonConfig \u001b[39m=\u001b[39m CommonConfig()\n\u001b[1;32m   1107\u001b[0m     common_eval: CommonEvalConfig \u001b[39m=\u001b[39m CommonEvalConfig()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/dataclasses.py:1230\u001b[0m, in \u001b[0;36mdataclass\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     \u001b[39mreturn\u001b[39;00m wrap\n\u001b[1;32m   1229\u001b[0m \u001b[39m# We're called as @dataclass without parens.\u001b[39;00m\n\u001b[0;32m-> 1230\u001b[0m \u001b[39mreturn\u001b[39;00m wrap(\u001b[39mcls\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/dataclasses.py:1220\u001b[0m, in \u001b[0;36mdataclass.<locals>.wrap\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39mcls\u001b[39m):\n\u001b[0;32m-> 1220\u001b[0m     \u001b[39mreturn\u001b[39;00m _process_class(\u001b[39mcls\u001b[39m, init, \u001b[39mrepr\u001b[39m, eq, order, unsafe_hash,\n\u001b[1;32m   1221\u001b[0m                           frozen, match_args, kw_only, slots,\n\u001b[1;32m   1222\u001b[0m                           weakref_slot)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/dataclasses.py:958\u001b[0m, in \u001b[0;36m_process_class\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m    955\u001b[0m         kw_only \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    956\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m         \u001b[39m# Otherwise it's a field of some type.\u001b[39;00m\n\u001b[0;32m--> 958\u001b[0m         cls_fields\u001b[39m.\u001b[39mappend(_get_field(\u001b[39mcls\u001b[39m, name, \u001b[39mtype\u001b[39m, kw_only))\n\u001b[1;32m    960\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m cls_fields:\n\u001b[1;32m    961\u001b[0m     fields[f\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m f\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/dataclasses.py:815\u001b[0m, in \u001b[0;36m_get_field\u001b[0;34m(cls, a_name, a_type, default_kw_only)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[39m# For real fields, disallow mutable defaults.  Use unhashable as a proxy\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[39m# indicator for mutability.  Read the __hash__ attribute from the class,\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[39m# not the instance.\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39m_field_type \u001b[39mis\u001b[39;00m _FIELD \u001b[39mand\u001b[39;00m f\u001b[39m.\u001b[39mdefault\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__hash__\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmutable default \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(f\u001b[39m.\u001b[39mdefault)\u001b[39m}\u001b[39;00m\u001b[39m for field \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    816\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m is not allowed: use default_factory\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    818\u001b[0m \u001b[39mreturn\u001b[39;00m f\n",
      "\u001b[0;31mValueError\u001b[0m: mutable default <class 'fairseq.dataclass.configs.CommonConfig'> for field common is not allowed: use default_factory"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "from musiclm_pytorch import MuLaN, MuLaNEmbedQuantizer, MuLaNTrainer, \\\n",
    "                            AudioSpectrogramTransformer, TextTransformer, MusicLM\n",
    "from audiolm_pytorch import SemanticTransformer, SemanticTransformerTrainer, \\\n",
    "                            CoarseTransformer, CoarseTransformerTrainer, \\\n",
    "                            FineTransformer, FineTransformerTrainer, \\\n",
    "                            AudioLM, HubertWithKmeans, MusicLMSoundStream, \\\n",
    "                            SoundStreamTrainer, SoundStream \n",
    "import os\n",
    "import wave\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating dataloaders and downloading Hubert K-means checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading Hubert checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hubert_ckpt = 'hubert/hubert_base_ls960.pt'\n",
    "hubert_quantizer = 'hubert/hubert_base_ls960_L9_km500.bin'\n",
    "soundstream_ckpt = './results/soundstream.pt'\n",
    "\n",
    "if not os.path.isdir(\"hubert\"):\n",
    "  os.makedirs(\"hubert\")\n",
    "if not os.path.isfile(hubert_ckpt):\n",
    "  hubert_ckpt_download = f\"https://dl.fbaipublicfiles.com/{hubert_ckpt}\"\n",
    "  urllib.request.urlretrieve(hubert_ckpt_download, f\"./{hubert_ckpt}\")\n",
    "if not os.path.isfile(hubert_quantizer):\n",
    "  hubert_quantizer_download = f\"https://dl.fbaipublicfiles.com/{hubert_quantizer}\"\n",
    "  urllib.request.urlretrieve(hubert_quantizer_download, f\"./{hubert_quantizer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training MuLaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments for every module are defined in the respective dictionaries to make fine-tuning easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m AUDIO_KWARGS \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdim\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m512\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdepth\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m6\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mspec_aug_stretch_factor\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.8\u001b[39m\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     11\u001b[0m TEXT_KWARGS \u001b[39m=\u001b[39m {\n\u001b[0;32m---> 12\u001b[0m     dim: \u001b[39m512\u001b[39m,\n\u001b[1;32m     13\u001b[0m     depth: \u001b[39m6\u001b[39m,\n\u001b[1;32m     14\u001b[0m     heads: \u001b[39m8\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdim_head\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m64\u001b[39m\n\u001b[1;32m     16\u001b[0m }\n\u001b[1;32m     18\u001b[0m audio_transformer \u001b[39m=\u001b[39m AudioSpectrogramTransformer(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mAUDIO_KWARGS)\n\u001b[1;32m     20\u001b[0m text_transformer \u001b[39m=\u001b[39m TextTransformer(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mTEXT_KWARGS)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dim' is not defined"
     ]
    }
   ],
   "source": [
    "AUDIO_KWARGS = {\n",
    "    'dim': 512,\n",
    "    'depth': 6,\n",
    "    'heads': 8,\n",
    "    'dim_head': 64,\n",
    "    'spec_n_fft': 128,\n",
    "    'spec_win_length': 24,\n",
    "    'spec_aug_stretch_factor': 0.8\n",
    "}\n",
    "\n",
    "TEXT_KWARGS = {\n",
    "    'dim': 512,\n",
    "    'depth': 6,\n",
    "    'heads': 8,\n",
    "    'dim_head': 64\n",
    "}\n",
    "\n",
    "MULAN_KWARGS = {\n",
    "    'folder': dataset_path,\n",
    "    'num_train_steps': 10,\n",
    "    'batch_size': 4,\n",
    "    'force_clear_prev_results': True,\n",
    "    'save_model_every': 5\n",
    "}\n",
    "\n",
    "MULAN_QUANTIZER_KWARGS = {\n",
    "    'conditioning_dims': (1024, 1024, 1024),\n",
    "    'namespaces': ('semantic', 'coarse', 'fine')\n",
    "}\n",
    "\n",
    "HUBERT_KWARGS = {\n",
    "    'checkpoint_path': hubert_ckpt,\n",
    "    'kmeans_path': hubert_quantizer\n",
    "}\n",
    "\n",
    "SOUNDSTREAM_TRAINER_KWARGS = {\n",
    "    'folder': dataset_path,\n",
    "    'num_train_steps': 20,\n",
    "    'save_model_every': 2,\n",
    "    'batch_size': 4\n",
    "}\n",
    "    \n",
    "SEMANTIC_KWARGS = {\n",
    "    'dim': 1024,\n",
    "    'depth': 6,\n",
    "    'audio_text_condition': True \n",
    "}\n",
    "\n",
    "COARSE_KWARGS = {\n",
    "    'codebook_size': 1024,\n",
    "    'num_coarse_quantizers': 4,\n",
    "    'dim': 1024,\n",
    "    'depth': 6,\n",
    "    'audio_text_condition': True \n",
    "}\n",
    "\n",
    "FINE_KWARGS = {\n",
    "    'codebook_size': 1024,\n",
    "    'num_coarse_quantizers': 4,\n",
    "    'num_fine_quantizers': 8,\n",
    "    'dim': 1024,\n",
    "    'depth': 6,\n",
    "    'audio_text_condition': True \n",
    "}\n",
    "\n",
    "TRANSFORMER_TRAINER_KWARGS = {\n",
    "    'folder': dataset_path,\n",
    "    'num_train_steps': 10,\n",
    "    'save_model_every': 2,\n",
    "    'batch_size': 4,\n",
    "    'data_max_length': 320 * 32\n",
    "}\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training MuLaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_transformer = AudioSpectrogramTransformer(**AUDIO_KWARGS)\n",
    "\n",
    "text_transformer = TextTransformer(**TEXT_KWARGS)\n",
    "\n",
    "mulan = MuLaN(audio_transformer, text_transformer)\n",
    "\n",
    "mulan_trainer = MuLaNTrainer(mulan, **MULAN_KWARGS)\n",
    "\n",
    "mulan_trainer.train()\n",
    "\n",
    "mulan_trainer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training SoundStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundstream = MusicLMSoundStream()\n",
    "soundstream_trainer = SoundStreamTrainer(\n",
    "    soundstream,\n",
    "    **SOUNDSTREAM_TRAINER_KWARGS\n",
    ")\n",
    "\n",
    "soundstream_trainer.train()\n",
    "\n",
    "soundstream_trainer.save(soundstream_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training conditioning embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the MuLaN Embed Quantizer and Hubert K-means Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = MuLaNEmbedQuantizer(\n",
    "    mulan=mulan,                         \n",
    "    **MULAN_QUANTIZER_KWARGS\n",
    ")\n",
    "\n",
    "wav2vec = HubertWithKmeans(\n",
    "    **HUBERT_KWARGS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Semantic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_transformer = SemanticTransformer(\n",
    "   num_semantic_tokens=wav2vec.codebook_size,\n",
    "   **SEMANTIC_KWARGS \n",
    ").to(DEVICE)\n",
    "\n",
    "semantic_trainer = SemanticTransformerTrainer(\n",
    "    wav2vec,\n",
    "    semantic_transformer,\n",
    "    audio_conditioner=quantizer,\n",
    "    **TRANSFORMER_TRAINER_KWARGS\n",
    ")\n",
    "\n",
    "semantic_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Coarse Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundstream = MusicLMSoundStream()\n",
    "\n",
    "soundstream.load(soundstream_ckpt)\n",
    "\n",
    "coarse_transformer = CoarseTransformer(\n",
    "    num_semantic_tokens=wav2vec.codebook_size,\n",
    "    **COARSE_KWARGS\n",
    ").to(DEVICE)\n",
    "\n",
    "coarse_trainer = CoarseTransformerTrainer(\n",
    "    wav2vec,\n",
    "    semantic_transformer,\n",
    "    codec=soundstream,\n",
    "    audio_conditioner=quantizer,\n",
    "    **TRANSFORMER_TRAINER_KWARGS\n",
    ")\n",
    "\n",
    "coarse_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Fine Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundstream = MusicLMSoundStream()\n",
    "\n",
    "soundstream.load(soundstream_ckpt)\n",
    "\n",
    "fine_transformer = FineTransformer(\n",
    "    codebook_size=wav2vec.codebook_size,\n",
    "    **FINE_KWARGS\n",
    ").to(DEVICE)\n",
    "\n",
    "fine_trainer = FineTransformerTrainer(\n",
    "    wav2vec,\n",
    "    semantic_transformer,\n",
    "    codec=soundstream\n",
    "    audio_conditioner=quantizer,\n",
    "    **TRANSFORMER_TRAINER_KWARGS\n",
    ")\n",
    "\n",
    "fine_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combining AudioLM and MusicLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_lm = AudioLM(\n",
    "    wav2vec=wav2vec,\n",
    "    codec=soundstream,\n",
    "    semantic_transformer=semantic_transformer,\n",
    "    coarse_transformer=coarse_transformer,\n",
    "    fine_transformer=fine_transformer   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_lm = MusicLM(\n",
    "    audio_lm=audio_lm,\n",
    "    mulan_embed_quantizer=quantizer\n",
    ")\n",
    "\n",
    "music = music_lm('soul', num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(music, 'generated_music.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"out.wav\"\n",
    "sample_rate = 44100\n",
    "torchaudio.save(output_path, music.cpu(), sample_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
