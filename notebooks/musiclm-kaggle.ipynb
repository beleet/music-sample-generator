{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training MusicLM model\n### Robert Chen, Ahmadsho Akdodshoev, Philip Timofeev","metadata":{}},{"cell_type":"markdown","source":"## 0. Imports","metadata":{}},{"cell_type":"code","source":"!pip install musiclm-pytorch","metadata":{"execution":{"iopub.status.busy":"2023-10-27T15:59:53.971282Z","iopub.execute_input":"2023-10-27T15:59:53.971723Z","iopub.status.idle":"2023-10-27T16:01:24.285187Z","shell.execute_reply.started":"2023-10-27T15:59:53.971688Z","shell.execute_reply":"2023-10-27T16:01:24.284055Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting musiclm-pytorch\n  Downloading musiclm_pytorch-0.2.8-py3-none-any.whl (14 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (0.22.0)\nCollecting audiolm-pytorch>=0.17.0 (from musiclm-pytorch)\n  Downloading audiolm_pytorch-1.6.3-py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting beartype (from musiclm-pytorch)\n  Downloading beartype-0.16.4-py3-none-any.whl (819 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.1/819.1 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting einops>=0.6 (from musiclm-pytorch)\n  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting lion-pytorch (from musiclm-pytorch)\n  Downloading lion_pytorch-0.1.2-py3-none-any.whl (4.4 kB)\nCollecting vector-quantize-pytorch>=1.0.0 (from musiclm-pytorch)\n  Downloading vector_quantize_pytorch-1.10.4-py3-none-any.whl (20 kB)\nCollecting x-clip (from musiclm-pytorch)\n  Downloading x_clip-0.14.4-py3-none-any.whl (1.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.12 in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (2.0.0+cpu)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (2.0.1+cpu)\nCollecting ema-pytorch>=0.2.2 (from audiolm-pytorch>=0.17.0->musiclm-pytorch)\n  Downloading ema_pytorch-0.2.4-py3-none-any.whl (4.4 kB)\nCollecting encodec (from audiolm-pytorch>=0.17.0->musiclm-pytorch)\n  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting fairseq (from audiolm-pytorch>=0.17.0->musiclm-pytorch)\n  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.3.2)\nCollecting local-attention>=1.9.0 (from audiolm-pytorch>=0.17.0->musiclm-pytorch)\n  Downloading local_attention-1.9.0-py3-none-any.whl (8.2 kB)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.2.2)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.1.99)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.33.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.66.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->musiclm-pytorch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->musiclm-pytorch) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->musiclm-pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->musiclm-pytorch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->musiclm-pytorch) (3.1.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate->musiclm-pytorch) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->musiclm-pytorch) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->musiclm-pytorch) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate->musiclm-pytorch) (6.0)\nCollecting ftfy (from x-clip->musiclm-pytorch)\n  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from x-clip->musiclm-pytorch) (2023.6.3)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from x-clip->musiclm-pytorch) (0.15.1+cpu)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate->musiclm-pytorch) (3.0.9)\nRequirement already satisfied: cffi in /opt/conda/lib/python3.10/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.15.1)\nRequirement already satisfied: cython in /opt/conda/lib/python3.10/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.29.35)\nCollecting hydra-core<1.1,>=1.0.7 (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch)\n  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting omegaconf<2.1 (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch)\n  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\nCollecting sacrebleu>=1.4.12 (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch)\n  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting bitarray (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch)\n  Downloading bitarray-2.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.5/286.5 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from ftfy->x-clip->musiclm-pytorch) (0.2.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.12->musiclm-pytorch) (2.1.3)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.11.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->audiolm-pytorch>=0.17.0->musiclm-pytorch) (3.1.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.12->musiclm-pytorch) (1.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->x-clip->musiclm-pytorch) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->x-clip->musiclm-pytorch) (9.5.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.16.4)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.3.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2023.9.0)\nCollecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch)\n  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting portalocker (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch)\n  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.9.3)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.21)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->x-clip->musiclm-pytorch) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->x-clip->musiclm-pytorch) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->x-clip->musiclm-pytorch) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->x-clip->musiclm-pytorch) (2023.7.22)\nBuilding wheels for collected packages: encodec, fairseq, antlr4-python3-runtime\n  Building wheel for encodec (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45760 sha256=e70efb2c426a4829ddd6c8794299aff6dc4ff653e31e01e87dadafcf5caf8b18\n  Stored in directory: /root/.cache/pip/wheels/fc/36/cb/81af8b985a5f5e0815312d5e52b41263237af07b977e6bcbf3\n  Building wheel for fairseq (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=10416084 sha256=f3bd2d980f5f58bc16b320160e344f3fe9197733f0cbf44cb8fb518ec8a4338b\n  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=ffaddd6b1765f5a689c88f17b327c0703aea970093b0f38a1bb6c086ba4fb735\n  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\nSuccessfully built encodec fairseq antlr4-python3-runtime\nInstalling collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, ftfy, einops, beartype, sacrebleu, hydra-core, vector-quantize-pytorch, local-attention, lion-pytorch, ema-pytorch, x-clip, fairseq, encodec, audiolm-pytorch, musiclm-pytorch\nSuccessfully installed antlr4-python3-runtime-4.8 audiolm-pytorch-1.6.3 beartype-0.16.4 bitarray-2.8.2 einops-0.7.0 ema-pytorch-0.2.4 encodec-0.1.1 fairseq-0.12.2 ftfy-6.1.1 hydra-core-1.0.7 lion-pytorch-0.1.2 local-attention-1.9.0 musiclm-pytorch-0.2.8 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.3.1 vector-quantize-pytorch-1.10.4 x-clip-0.14.4\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchaudio\nfrom musiclm_pytorch import MuLaN, MuLaNEmbedQuantizer, MuLaNTrainer, \\\n                            AudioSpectrogramTransformer, TextTransformer, MusicLM\nfrom audiolm_pytorch import SemanticTransformer, SemanticTransformerTrainer, \\\n                            CoarseTransformer, CoarseTransformerTrainer, \\\n                            FineTransformer, FineTransformerTrainer, \\\n                            AudioLM, HubertWithKmeans, MusicLMSoundStream, \\\n                            SoundStreamTrainer, SoundStream \nimport os\nimport wave\nimport urllib.request","metadata":{"execution":{"iopub.status.busy":"2023-10-27T16:01:24.288127Z","iopub.execute_input":"2023-10-27T16:01:24.288559Z","iopub.status.idle":"2023-10-27T16:02:27.387628Z","shell.execute_reply.started":"2023-10-27T16:01:24.288516Z","shell.execute_reply":"2023-10-27T16:02:27.386498Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## 1. Creating dataloaders and downloading Hubert K-means checkpoints","metadata":{}},{"cell_type":"markdown","source":"Creating the dataset","metadata":{}},{"cell_type":"code","source":"dataset_path = '/kaggle/input/soul-mzk/'\n\nclass MusicLMDataset(Dataset):\n    def __init__(self, path: str):\n        pass\n    def __getitem__(self, idx):\n        pass\n    \ntrain_dataset = MusicLMDataset(dataset_path)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T16:02:27.389026Z","iopub.execute_input":"2023-10-27T16:02:27.389764Z","iopub.status.idle":"2023-10-27T16:02:27.394858Z","shell.execute_reply.started":"2023-10-27T16:02:27.389731Z","shell.execute_reply":"2023-10-27T16:02:27.393919Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Downloading Hubert checkpoints","metadata":{}},{"cell_type":"code","source":"hubert_ckpt = 'hubert/hubert_base_ls960.pt'\nhubert_quantizer = 'hubert/hubert_base_ls960_L9_km500.bin'\nsoundstream_ckpt = './results/soundstream.pt'\n\nif not os.path.isdir(\"hubert\"):\n  os.makedirs(\"hubert\")\nif not os.path.isfile(hubert_ckpt):\n  hubert_ckpt_download = f\"https://dl.fbaipublicfiles.com/{hubert_ckpt}\"\n  urllib.request.urlretrieve(hubert_ckpt_download, f\"./{hubert_ckpt}\")\nif not os.path.isfile(hubert_quantizer):\n  hubert_quantizer_download = f\"https://dl.fbaipublicfiles.com/{hubert_quantizer}\"\n  urllib.request.urlretrieve(hubert_quantizer_download, f\"./{hubert_quantizer}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T16:02:27.397349Z","iopub.execute_input":"2023-10-27T16:02:27.397774Z","iopub.status.idle":"2023-10-27T16:02:32.950664Z","shell.execute_reply.started":"2023-10-27T16:02:27.397741Z","shell.execute_reply":"2023-10-27T16:02:32.949223Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## 2. Training MuLaN","metadata":{}},{"cell_type":"markdown","source":"Arguments for every module are defined in the respective dictionaries to make fine-tuning easier","metadata":{}},{"cell_type":"code","source":"AUDIO_KWARGS = {\n    'dim': 512,\n    'depth': 6,\n    'heads': 8,\n    'dim_head': 64,\n    'spec_n_fft': 128,\n    'spec_win_length': 24,\n    'spec_aug_stretch_factor': 0.8\n}\n\nTEXT_KWARGS = {\n    'dim': 512,\n    'depth': 6,\n    'heads': 8,\n    'dim_head': 64\n}\n\nMULAN_KWARGS = {\n    'dataset': train_dataset\n    'num_train_steps': 10,\n    'batch_size': 4,\n    'force_clear_prev_results': True,\n    'save_model_every': 5\n}\n\nMULAN_QUANTIZER_KWARGS = {\n    'conditioning_dims': (1024, 1024, 1024),\n    'namespaces': ('semantic', 'coarse', 'fine')\n}\n\nHUBERT_KWARGS = {\n    'checkpoint_path': hubert_ckpt,\n    'kmeans_path': hubert_quantizer\n}\n\nSOUNDSTREAM_TRAINER_KWARGS = {\n    'folder': dataset_path,\n    'num_train_steps': 20,\n    'save_model_every': 2,\n    'batch_size': 4\n}\n    \nSEMANTIC_KWARGS = {\n    'dim': 1024,\n    'depth': 6,\n    'audio_text_condition': True \n}\n\nCOARSE_KWARGS = {\n    'codebook_size': 1024,\n    'num_coarse_quantizers': 4,\n    'dim': 1024,\n    'depth': 6,\n    'audio_text_condition': True \n}\n\nFINE_KWARGS = {\n    'codebook_size': 1024,\n    'num_coarse_quantizers': 4,\n    'num_fine_quantizers': 8,\n    'dim': 1024,\n    'depth': 6,\n    'audio_text_condition': True \n}\n\nTRANSFORMER_TRAINER_KWARGS = {\n    'folder': dataset_path,\n    'num_train_steps': 10,\n    'save_model_every': 2,\n    'batch_size': 4,\n    'data_max_length': 320 * 32\n}\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-10-27T16:02:32.954435Z","iopub.execute_input":"2023-10-27T16:02:32.954822Z","iopub.status.idle":"2023-10-27T16:02:32.968778Z","shell.execute_reply.started":"2023-10-27T16:02:32.954790Z","shell.execute_reply":"2023-10-27T16:02:32.966863Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Training MuLaN","metadata":{}},{"cell_type":"code","source":"audio_transformer = AudioSpectrogramTransformer(**AUDIO_KWARGS)\n\ntext_transformer = TextTransformer(**TEXT_KWARGS)\n\nmulan = MuLaN(audio_transformer, text_transformer)\n\nmulan_trainer = MuLaNTrainer(mulan, **MULAN_KWARGS)\n\nmulan_trainer.train()\n\nmulan_trainer.save()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T16:02:32.970888Z","iopub.execute_input":"2023-10-27T16:02:32.971542Z","iopub.status.idle":"2023-10-27T16:02:34.268257Z","shell.execute_reply.started":"2023-10-27T16:02:32.971491Z","shell.execute_reply":"2023-10-27T16:02:34.266103Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m text_transformer \u001b[38;5;241m=\u001b[39m TextTransformer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mTEXT_KWARGS)\n\u001b[1;32m      5\u001b[0m mulan \u001b[38;5;241m=\u001b[39m MuLaN(audio_transformer, text_transformer)\n\u001b[0;32m----> 7\u001b[0m mulan_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mMuLaNTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmulan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mMULAN_KWARGS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m mulan_trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     11\u001b[0m mulan_trainer\u001b[38;5;241m.\u001b[39msave()\n","File \u001b[0;32m<@beartype(musiclm_pytorch.trainer.MuLaNTrainer.__init__) at 0x7af57122b7f0>:68\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(__beartype_func, __beartype_conf, __beartype_get_violation, __beartype_object_102282399561792, __beartype_object_102282275442288, *args, **kwargs)\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: MuLaNTrainer.__init__() missing 1 required positional argument: 'dataset'"],"ename":"TypeError","evalue":"MuLaNTrainer.__init__() missing 1 required positional argument: 'dataset'","output_type":"error"}]},{"cell_type":"markdown","source":"## 3. Training SoundStream","metadata":{}},{"cell_type":"code","source":"soundstream = MusicLMSoundStream()\nsoundstream_trainer = SoundStreamTrainer(\n    soundstream,\n    **SOUNDSTREAM_TRAINER_KWARGS\n)\n\nsoundstream_trainer.train()\n\nsoundstream_trainer.save(soundstream_ckpt)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T16:02:34.269812Z","iopub.status.idle":"2023-10-27T16:02:34.270272Z","shell.execute_reply.started":"2023-10-27T16:02:34.270065Z","shell.execute_reply":"2023-10-27T16:02:34.270086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Training conditioning embeddings","metadata":{}},{"cell_type":"markdown","source":"Defining the MuLaN Embed Quantizer and Hubert K-means Embedder","metadata":{}},{"cell_type":"code","source":"quantizer = MuLaNEmbedQuantizer(\n    mulan=mulan,                         \n    **MULAN_QUANTIZER_KWARGS\n)\n\nwav2vec = HubertWithKmeans(\n    **HUBERT_KWARGS\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T16:02:34.271752Z","iopub.status.idle":"2023-10-27T16:02:34.272205Z","shell.execute_reply.started":"2023-10-27T16:02:34.271996Z","shell.execute_reply":"2023-10-27T16:02:34.272017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training Semantic Transformer","metadata":{}},{"cell_type":"code","source":"semantic_transformer = SemanticTransformer(\n   num_semantic_tokens=wav2vec.codebook_size,\n   **SEMANTIC_KWARGS \n).to(DEVICE)\n\nsemantic_trainer = SemanticTransformerTrainer(\n    wav2vec,\n    semantic_transformer,\n    audio_conditioner=quantizer,\n    **TRANSFORMER_TRAINER_KWARGS\n)\n\nsemantic_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T16:02:34.273280Z","iopub.status.idle":"2023-10-27T16:02:34.273734Z","shell.execute_reply.started":"2023-10-27T16:02:34.273525Z","shell.execute_reply":"2023-10-27T16:02:34.273546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training Coarse Transformer","metadata":{}},{"cell_type":"code","source":"soundstream = MusicLMSoundStream()\n\nsoundstream.load(soundstream_ckpt)\n\ncoarse_transformer = CoarseTransformer(\n    num_semantic_tokens=wav2vec.codebook_size,\n    **COARSE_KWARGS\n).to(DEVICE)\n\ncoarse_trainer = CoarseTransformerTrainer(\n    wav2vec,\n    semantic_transformer,\n    codec=soundstream,\n    audio_conditioner=quantizer,\n    **TRANSFORMER_TRAINER_KWARGS\n)\n\ncoarse_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T16:02:34.275271Z","iopub.status.idle":"2023-10-27T16:02:34.275734Z","shell.execute_reply.started":"2023-10-27T16:02:34.275518Z","shell.execute_reply":"2023-10-27T16:02:34.275539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training Fine Transformer","metadata":{}},{"cell_type":"code","source":"soundstream = MusicLMSoundStream()\n\nsoundstream.load(soundstream_ckpt)\n\nfine_transformer = FineTransformer(\n    codebook_size=wav2vec.codebook_size,\n    **FINE_KWARGS\n).to(DEVICE)\n\nfine_trainer = FineTransformerTrainer(\n    wav2vec,\n    semantic_transformer,\n    codec=soundstream\n    audio_conditioner=quantizer,\n    **TRANSFORMER_TRAINER_KWARGS\n)\n\nfine_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T16:02:34.277656Z","iopub.status.idle":"2023-10-27T16:02:34.279050Z","shell.execute_reply.started":"2023-10-27T16:02:34.278695Z","shell.execute_reply":"2023-10-27T16:02:34.278732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Combining AudioLM and MusicLM","metadata":{}},{"cell_type":"code","source":"audio_lm = AudioLM(\n    wav2vec=wav2vec,\n    codec=soundstream,\n    semantic_transformer=semantic_transformer,\n    coarse_transformer=coarse_transformer,\n    fine_transformer=fine_transformer   \n)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T16:02:34.280106Z","iopub.status.idle":"2023-10-27T16:02:34.280579Z","shell.execute_reply.started":"2023-10-27T16:02:34.280335Z","shell.execute_reply":"2023-10-27T16:02:34.280354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"music_lm = MusicLM(\n    audio_lm=audio_lm,\n    mulan_embed_quantizer=quantizer\n)\n\nmusic = music_lm('soul', num_samples=1)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T16:02:34.282995Z","iopub.status.idle":"2023-10-27T16:02:34.283496Z","shell.execute_reply.started":"2023-10-27T16:02:34.283238Z","shell.execute_reply":"2023-10-27T16:02:34.283259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(music, 'generated_music.pt')","metadata":{"execution":{"iopub.status.busy":"2023-10-27T16:02:34.285963Z","iopub.status.idle":"2023-10-27T16:02:34.286397Z","shell.execute_reply.started":"2023-10-27T16:02:34.286185Z","shell.execute_reply":"2023-10-27T16:02:34.286203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_path = \"out.wav\"\nsample_rate = 44100\ntorchaudio.save(output_path, music.cpu(), sample_rate)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T16:02:34.288113Z","iopub.status.idle":"2023-10-27T16:02:34.288593Z","shell.execute_reply.started":"2023-10-27T16:02:34.288327Z","shell.execute_reply":"2023-10-27T16:02:34.288347Z"},"trusted":true},"execution_count":null,"outputs":[]}]}