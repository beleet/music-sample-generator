{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7030475,"sourceType":"datasetVersion","datasetId":4043676},{"sourceId":7031158,"sourceType":"datasetVersion","datasetId":3918425}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training MusicLM model\n### Robert Chen, Ahmadsho Akdodshoev, Philip Timofeev","metadata":{}},{"cell_type":"markdown","source":"## 0. Imports","metadata":{}},{"cell_type":"code","source":"!pip install musiclm-pytorch","metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:23:20.342228Z","iopub.execute_input":"2023-11-23T03:23:20.342753Z","iopub.status.idle":"2023-11-23T03:23:35.140123Z","shell.execute_reply.started":"2023-11-23T03:23:20.342712Z","shell.execute_reply":"2023-11-23T03:23:35.138728Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"Requirement already satisfied: musiclm-pytorch in /opt/conda/lib/python3.10/site-packages (0.2.8)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (0.24.1)\nRequirement already satisfied: audiolm-pytorch>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (1.8.2)\nRequirement already satisfied: beartype in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (0.16.4)\nRequirement already satisfied: einops>=0.6 in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (0.7.0)\nRequirement already satisfied: lion-pytorch in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (0.1.2)\nRequirement already satisfied: vector-quantize-pytorch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (1.11.7)\nRequirement already satisfied: x-clip in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (0.14.4)\nRequirement already satisfied: torch>=1.12 in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (2.0.0+cpu)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (2.0.1+cpu)\nRequirement already satisfied: ema-pytorch>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.3.1)\nRequirement already satisfied: encodec in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.1.1)\nRequirement already satisfied: fairseq in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.12.2)\nRequirement already satisfied: gateloop-transformer>=0.0.24 in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.1.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.3.2)\nRequirement already satisfied: local-attention>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.9.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.2.2)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.1.99)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.33.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.66.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate->musiclm-pytorch) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->musiclm-pytorch) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->musiclm-pytorch) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate->musiclm-pytorch) (6.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate->musiclm-pytorch) (0.16.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->musiclm-pytorch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->musiclm-pytorch) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->musiclm-pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->musiclm-pytorch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->musiclm-pytorch) (3.1.2)\nRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from x-clip->musiclm-pytorch) (6.1.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from x-clip->musiclm-pytorch) (2023.6.3)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from x-clip->musiclm-pytorch) (0.15.1+cpu)\nRequirement already satisfied: rotary-embedding-torch in /opt/conda/lib/python3.10/site-packages (from gateloop-transformer>=0.0.24->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.3.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate->musiclm-pytorch) (3.0.9)\nRequirement already satisfied: cffi in /opt/conda/lib/python3.10/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.15.1)\nRequirement already satisfied: cython in /opt/conda/lib/python3.10/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.29.35)\nRequirement already satisfied: hydra-core<1.1,>=1.0.7 in /opt/conda/lib/python3.10/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.0.7)\nRequirement already satisfied: omegaconf<2.1 in /opt/conda/lib/python3.10/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.0.6)\nRequirement already satisfied: sacrebleu>=1.4.12 in /opt/conda/lib/python3.10/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.3.2)\nRequirement already satisfied: bitarray in /opt/conda/lib/python3.10/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.8.3)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->x-clip->musiclm-pytorch) (0.2.12)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate->musiclm-pytorch) (2023.9.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate->musiclm-pytorch) (2.31.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.12->musiclm-pytorch) (2.1.3)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.11.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->audiolm-pytorch>=0.17.0->musiclm-pytorch) (3.1.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.12->musiclm-pytorch) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->x-clip->musiclm-pytorch) (9.5.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.3.3)\nRequirement already satisfied: antlr4-python3-runtime==4.8 in /opt/conda/lib/python3.10/site-packages (from hydra-core<1.1,>=1.0.7->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.8)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.8.2)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.9.3)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.21)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate->musiclm-pytorch) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate->musiclm-pytorch) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate->musiclm-pytorch) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate->musiclm-pytorch) (2023.7.22)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchaudio\nfrom musiclm_pytorch import MuLaN, MuLaNEmbedQuantizer, \\\n                            AudioSpectrogramTransformer, TextTransformer, MusicLM\nfrom audiolm_pytorch import SemanticTransformer, SemanticTransformerTrainer, \\\n                            CoarseTransformer, CoarseTransformerTrainer, \\\n                            FineTransformer, FineTransformerTrainer, \\\n                            AudioLM, HubertWithKmeans, MusicLMSoundStream, \\\n                            SoundStreamTrainer, SoundStream \nimport os\nfrom scipy.io.wavfile import read as read_wav\nimport urllib.request\nimport pandas as pd\nimport numpy as np\nimport audio2numpy as a2n\nfrom x_clip.tokenizer import tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:23:35.143678Z","iopub.execute_input":"2023-11-23T03:23:35.144251Z","iopub.status.idle":"2023-11-23T03:23:35.152791Z","shell.execute_reply.started":"2023-11-23T03:23:35.144193Z","shell.execute_reply":"2023-11-23T03:23:35.151601Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"## 1. Creating dataloaders and downloading Hubert K-means checkpoints","metadata":{}},{"cell_type":"markdown","source":"Creating the dataset","metadata":{}},{"cell_type":"code","source":"dataset_path = '/kaggle/input/musiclm-test/music-lm/data/dataset.tsv'\naudio_path = '/kaggle/input/musiclm-test/music-lm/data/'\n\n\nclass MusicLMDataset(Dataset):\n    def __init__(self, path: str):\n        self.df = pd.read_csv(path, sep='\\t')\n        self.filenames = list(map(lambda x: audio_path + x, self.df['filename']))\n        self.authors = self.df['author']\n        self.years = self.df['year']\n    def __getitem__(self, idx):\n        return torch.tensor(a2n.audio_from_file(self.filenames[idx])[0]), tokenizer.tokenize([self.authors[idx], self.years[idx]]).reshape(-1)\n    def __len__(self):\n        return len(self.filenames)\n    \ntrain_dataset = MusicLMDataset(dataset_path)\ntrain_dataloader = DataLoader(train_dataset, batch_size=3)\ntmp = next(iter(train_dataloader))\nprint(tmp[1].shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:23:35.154254Z","iopub.execute_input":"2023-11-23T03:23:35.155619Z","iopub.status.idle":"2023-11-23T03:23:35.589124Z","shell.execute_reply.started":"2023-11-23T03:23:35.155579Z","shell.execute_reply":"2023-11-23T03:23:35.587707Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"torch.Size([3, 10])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Downloading Hubert checkpoints","metadata":{}},{"cell_type":"code","source":"hubert_ckpt = 'hubert/hubert_base_ls960.pt'\nhubert_quantizer = 'hubert/hubert_base_ls960_L9_km500.bin'\nsoundstream_ckpt = './results/soundstream.pt'\nmulan_ckpt = './results/mulan.pt'\n\nif not os.path.isdir(\"hubert\"):\n  os.makedirs(\"hubert\")\nif not os.path.isfile(hubert_ckpt):\n  hubert_ckpt_download = f\"https://dl.fbaipublicfiles.com/{hubert_ckpt}\"\n  urllib.request.urlretrieve(hubert_ckpt_download, f\"./{hubert_ckpt}\")\nif not os.path.isfile(hubert_quantizer):\n  hubert_quantizer_download = f\"https://dl.fbaipublicfiles.com/{hubert_quantizer}\"\n  urllib.request.urlretrieve(hubert_quantizer_download, f\"./{hubert_quantizer}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:23:35.591778Z","iopub.execute_input":"2023-11-23T03:23:35.592498Z","iopub.status.idle":"2023-11-23T03:23:35.600126Z","shell.execute_reply.started":"2023-11-23T03:23:35.592452Z","shell.execute_reply":"2023-11-23T03:23:35.598893Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"## 2. Training MuLaN","metadata":{}},{"cell_type":"markdown","source":"Arguments for every module are defined in the respective dictionaries to make fine-tuning easier","metadata":{}},{"cell_type":"code","source":"AUDIO_KWARGS = {\n    'dim': 512,\n    'depth': 6,\n    'heads': 8,\n    'accept_spec': True,\n    'dim_head': 64,\n    'spec_n_fft': 128,\n    'spec_win_length': 24,\n    'spec_aug_stretch_factor': 0.8,\n    'patch_dropout_prob': 0.\n}\n\nTEXT_KWARGS = {\n    'dim': 512,\n    'depth': 6,\n    'heads': 8,\n    'dim_head': 64\n}\n\nMULAN_KWARGS = {\n    'dataset': train_dataset,\n    'num_train_steps': 10,\n    'batch_size': 16,\n    'force_clear_prev_results': False,\n    'save_model_every': 5\n}\n\nMULAN_QUANTIZER_KWARGS = {\n    'conditioning_dims': (1024, 1024, 1024),\n    'namespaces': ('semantic', 'coarse', 'fine')\n}\n\nHUBERT_KWARGS = {\n    'checkpoint_path': hubert_ckpt,\n    'kmeans_path': hubert_quantizer\n}\n\nSOUNDSTREAM_TRAINER_KWARGS = {\n    'folder': audio_path,\n    'num_train_steps': 20,\n    'save_model_every': 2,\n    'batch_size': 4,\n    'data_max_length_seconds': 60\n}\n    \nSEMANTIC_KWARGS = {\n    'dim': 1024,\n    'depth': 6,\n    'audio_text_condition': True \n}\n\nCOARSE_KWARGS = {\n    'codebook_size': 1024,\n    'num_coarse_quantizers': 4,\n    'dim': 1024,\n    'depth': 6,\n    'audio_text_condition': True \n}\n\nFINE_KWARGS = {\n    'codebook_size': 1024,\n    'num_coarse_quantizers': 4,\n    'num_fine_quantizers': 8,\n    'dim': 1024,\n    'depth': 6,\n    'audio_text_condition': True \n}\n\nTRANSFORMER_TRAINER_KWARGS = {\n    'folder': audio_path,\n    'num_train_steps': 10,\n    'save_model_every': 2,\n    'batch_size': 4,\n    'data_max_length': 320 * 32\n}\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:23:35.601411Z","iopub.execute_input":"2023-11-23T03:23:35.602470Z","iopub.status.idle":"2023-11-23T03:23:35.614788Z","shell.execute_reply.started":"2023-11-23T03:23:35.602433Z","shell.execute_reply":"2023-11-23T03:23:35.613777Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"Training MuLaN","metadata":{}},{"cell_type":"code","source":"import copy\nfrom math import sqrt\nfrom random import choice\nfrom pathlib import Path\nfrom shutil import rmtree\nfrom functools import wraps, partial\n\nfrom typing_extensions import Annotated\n\nfrom beartype import beartype\nfrom beartype.door import is_bearable\nfrom beartype.vale import Is\nfrom beartype.typing import Union, List, Optional, Tuple, Callable\n\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom lion_pytorch import Lion\n\nfrom musiclm_pytorch import MuLaN\n\nfrom einops import rearrange\n\nfrom accelerate import Accelerator\n\n# for automatically routing data emitted from a dataset to keywords of the transformer wrappers\n\nDATASET_FIELD_TYPE_CONFIG = dict(\n    wavs = Annotated[\n        torch.Tensor,\n        Is[lambda t: t.dtype == torch.float and t.ndim in {2, 3}]\n    ],\n    raw_texts = List[str],\n    texts = Annotated[\n        torch.Tensor,\n        Is[lambda t: t.dtype == torch.long and t.ndim == 2]\n    ],\n)\n\n# helpers\n\ndef exists(val):\n    return val is not None\n\ndef default(*args):\n    for arg in args:\n        if exists(arg):\n            return arg\n    return None\n\ndef noop(*args, **kwargs):\n    pass\n\ndef cycle(dl):\n    while True:\n        for data in dl:\n            yield data\n\ndef cast_tuple(t):\n    return t if isinstance(t, (tuple, list)) else (t,)\n\ndef yes_or_no(question):\n    answer = input(f'{question} (y/n) ')\n    return answer.lower() in ('yes', 'y')\n\ndef accum_log(log, new_logs):\n    for key, new_value in new_logs.items():\n        old_value = log.get(key, 0.)\n        log[key] = old_value + new_value\n    return log\n\n# auto data to module keyword argument routing functions\n\ndef has_duplicates(tup):\n    counts = dict()\n    for el in tup:\n        if el not in counts:\n            counts[el] = 0\n        counts[el] += 1\n    return any(filter(lambda count: count > 1, counts.values()))\n\ndef determine_types(data, config):\n    output = []\n    for el in data:\n        for name, data_type in config.items():\n            if is_bearable(el, data_type):\n                output.append(name)\n                break\n        else:\n            raise TypeError(f'unable to determine type of {data}')\n\n    return tuple(output)\n\n# optimizer functions\n\ndef separate_weight_decayable_params(params):\n    wd_params, no_wd_params = [], []\n    for param in params:\n        param_list = no_wd_params if param.ndim < 2 else wd_params\n        param_list.append(param)\n    return wd_params, no_wd_params\n\n# dataloader functions\n\ndef collate_one_or_multiple_tensors(fn):\n    @wraps(fn)\n    def inner(data):\n        is_one_data = not isinstance(data[0], tuple)\n\n        if is_one_data:\n            data = torch.stack(data)\n            return (data,)\n\n        outputs = []\n        for datum in zip(*data):\n            if is_bearable(datum, Tuple[str, ...]):\n                output = list(datum)\n            else:\n                output = fn(datum)\n\n            outputs.append(output)\n\n        return tuple(outputs)\n\n    return inner\n\n@collate_one_or_multiple_tensors\ndef curtail_to_shortest_collate(data):\n    min_len = min(*[datum.shape[0] for datum in data])\n    data = [datum[:min_len] for datum in data]\n    return torch.stack(data)\n\n@collate_one_or_multiple_tensors\ndef pad_to_longest_fn(data):\n    return pad_sequence(data, batch_first = True)\n\ndef get_dataloader(ds, pad_to_longest = True, **kwargs):\n    collate_fn = pad_to_longest_fn if pad_to_longest else curtail_to_shortest_collate\n    return DataLoader(ds, collate_fn = collate_fn, **kwargs)\n\n# semantic transformer trainer\n\n@beartype\nclass MuLaNTrainer(nn.Module):\n    def __init__(\n        self,\n        mulan: MuLaN,\n        dataset: Dataset,\n        *,\n        num_train_steps = None,\n        batch_size,\n        data_max_length = None,\n        folder = None,\n        lr = 3e-4,\n        grad_accum_every = 1,\n        betas = (0.9, 0.99),\n        max_grad_norm = 0.5,\n        valid_frac = 0.05,\n        random_split_seed = 42,\n        save_model_every = 1000,\n        results_folder = './results',\n        accelerate_kwargs: dict = dict(),\n        use_lion = False,\n        force_clear_prev_results = None  # set to True | False to skip the prompt\n    ):\n        super().__init__()\n        assert batch_size > 1, 'batch size must be greater than 1 for contrastive learning (but ideally as large as possible)'\n\n        self.accelerator = Accelerator(**accelerate_kwargs)\n\n        self.mulan = mulan\n\n        self.register_buffer('steps', torch.Tensor([0]))\n\n        self.num_train_steps = default(num_train_steps, len(dataset)) # 1 epoch by default\n        self.batch_size = batch_size\n        self.grad_accum_every = grad_accum_every\n\n        # optimizers\n\n        optim_klass = Lion if use_lion else Adam\n        self.optim = optim_klass(mulan.parameters(), lr = lr, betas = betas)\n\n        # max grad norm\n\n        self.max_grad_norm = max_grad_norm\n\n        self.data_max_length = data_max_length\n\n        # create dataset\n\n        self.ds = dataset\n        self.ds_fields = None\n\n        # split for validation\n\n        if valid_frac > 0:\n            train_size = int((1 - valid_frac) * len(self.ds))\n            valid_size = len(self.ds) - train_size\n            self.ds, self.valid_ds = random_split(self.ds, [train_size, valid_size], generator = torch.Generator().manual_seed(random_split_seed))\n            self.print(f'training with dataset of {len(self.ds)} samples and validating with randomly splitted {len(self.valid_ds)} samples')\n        else:\n            self.valid_ds = self.ds\n            self.print(f'training with shared training and valid dataset of {len(self.ds)} samples')\n\n        # dataloader\n\n        self.dl = get_dataloader(self.ds, batch_size = batch_size, shuffle = True, pad_to_longest = False, drop_last = True)\n\n        self.valid_dl = get_dataloader(self.valid_ds, batch_size = batch_size, shuffle = True, pad_to_longest = False, drop_last = True)\n\n        # prepare with accelerator\n\n        (\n            self.mulan,\n            self.optim,\n            self.dl,\n            self.valid_dl\n        ) = self.accelerator.prepare(\n            self.mulan,\n            self.optim,\n            self.dl,\n            self.valid_dl\n        )\n\n        # dataloader iterators\n\n        self.dl_iter = cycle(self.dl)\n        self.valid_dl_iter = cycle(self.valid_dl)\n\n        self.save_model_every = save_model_every\n\n        hps = dict(\n            num_train_steps = num_train_steps,\n            data_max_length = data_max_length,\n            learning_rate = lr\n        )\n\n        self.accelerator.init_trackers(\"mulan\", config = hps)\n\n        # results folder\n\n        self.results_folder = Path(results_folder)\n\n        if force_clear_prev_results is True or (not exists(force_clear_prev_results) and len([*self.results_folder.glob('**/*')]) > 0 and yes_or_no('do you want to clear previous experiment checkpoints and results?')):\n            rmtree(str(self.results_folder))\n\n        self.results_folder.mkdir(parents = True, exist_ok = True)\n\n        # to device\n\n        self.mulan.to(self.device)\n\n    def save(self, path):\n        pkg = dict(\n            model = self.accelerator.get_state_dict(self.mulan),\n            optim = self.optim.state_dict()\n        )\n        torch.save(pkg, path)\n\n    def load(self, path):\n        path = Path(path)\n        assert path.exists()\n        pkg = torch.load(str(path), map_location = 'cpu')\n\n        mulan = self.accelerator.unwrap_model(self.mulan)\n        mulan.load_state_dict(pkg['model'])\n        self.optim.load_state_dict(pkg['optim'])\n\n    def print(self, msg):\n        self.accelerator.print(msg)\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    def data_tuple_to_kwargs(self, data):\n        data_kwargs = {'wavs': data[0], 'texts': data[1]}\n\n        return data_kwargs\n\n    def train_step(self):\n        device = self.device\n\n        steps = int(self.steps.item())\n\n        self.mulan.train()\n\n        # logs\n\n        logs = {}\n\n        # update vae (generator)\n\n        for _ in range(self.grad_accum_every):\n            data_kwargs = self.data_tuple_to_kwargs(next(self.dl_iter))\n\n            loss = self.mulan(**data_kwargs)\n\n            self.accelerator.backward(loss / self.grad_accum_every)\n\n            accum_log(logs, {'loss': loss.item() / self.grad_accum_every})\n\n        if exists(self.max_grad_norm):\n            self.accelerator.clip_grad_norm_(self.mulan.parameters(), self.max_grad_norm)\n\n        self.optim.step()\n        self.optim.zero_grad()\n\n        # log\n\n        self.print(f\"{steps}: loss: {logs['loss']}\")\n        self.accelerator.log({\"train_loss\": logs['loss']}, step = steps)\n\n        # save model every so often\n\n        if self.is_main and not (steps % self.save_model_every):\n            model_path = str(self.results_folder / f'mulan.{steps}.pt')\n            self.save(model_path)\n\n            self.print(f'{steps}: saving model to {str(self.results_folder)}')\n\n        self.steps += 1\n        return logs\n\n    def train(self, log_fn: Callable = noop):\n\n        while self.steps < self.num_train_steps:\n            logs = self.train_step()\n            log_fn(logs)\n\n        self.print('training complete')","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-11-23T03:23:35.616613Z","iopub.execute_input":"2023-11-23T03:23:35.617303Z","iopub.status.idle":"2023-11-23T03:23:35.678069Z","shell.execute_reply.started":"2023-11-23T03:23:35.617257Z","shell.execute_reply":"2023-11-23T03:23:35.676781Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"audio_transformer = AudioSpectrogramTransformer(**AUDIO_KWARGS)\n\ntext_transformer = TextTransformer(**TEXT_KWARGS)\n\nmulan = MuLaN(audio_transformer, text_transformer)\n\nmulan_trainer = MuLaNTrainer(mulan, **MULAN_KWARGS)\n\nmulan_trainer.train()\n\nmulan_trainer.save(mulan_ckpt)\n\ndel mulan_trainer","metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:23:35.679262Z","iopub.execute_input":"2023-11-23T03:23:35.680677Z","iopub.status.idle":"2023-11-23T03:24:18.656348Z","shell.execute_reply.started":"2023-11-23T03:23:35.680638Z","shell.execute_reply":"2023-11-23T03:24:18.653201Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"training with dataset of 7214 samples and validating with randomly splitted 380 samples\n0: loss: nan\n0: saving model to results\n1: loss: nan\n2: loss: nan\n3: loss: nan\n4: loss: nan\n5: loss: nan\n5: saving model to results\n6: loss: nan\n7: loss: nan\n8: loss: nan\n9: loss: nan\ntraining complete\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3. Training SoundStream","metadata":{}},{"cell_type":"code","source":"soundstream = MusicLMSoundStream()\nsoundstream_trainer = SoundStreamTrainer(\n    soundstream,\n    **SOUNDSTREAM_TRAINER_KWARGS\n)\n\nsoundstream_trainer.train()\n\nsoundstream_trainer.save(soundstream_ckpt)\n\ndel soundstream_trainer","metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:24:18.662280Z","iopub.execute_input":"2023-11-23T03:24:18.663459Z","iopub.status.idle":"2023-11-23T03:24:19.831525Z","shell.execute_reply.started":"2023-11-23T03:24:18.663291Z","shell.execute_reply":"2023-11-23T03:24:19.828437Z"},"trusted":true},"execution_count":78,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[78], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m soundstream \u001b[38;5;241m=\u001b[39m MusicLMSoundStream()\n\u001b[0;32m----> 2\u001b[0m soundstream_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSoundStreamTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoundstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mSOUNDSTREAM_TRAINER_KWARGS\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m soundstream_trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      9\u001b[0m soundstream_trainer\u001b[38;5;241m.\u001b[39msave(soundstream_ckpt)\n","File \u001b[0;32m<@beartype(audiolm_pytorch.trainer.SoundStreamTrainer.__init__) at 0x79e74bdd1e10>:460\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(__beartype_func, __beartype_conf, __beartype_get_violation, __beartype_object_102373572347344, __beartype_object_134034319916096, __beartype_object_102373363221952, __beartype_object_102373453351520, __beartype_object_134034319915904, *args, **kwargs)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/audiolm_pytorch/trainer.py:200\u001b[0m, in \u001b[0;36mSoundStreamTrainer.__init__\u001b[0;34m(self, soundstream, num_train_steps, batch_size, data_max_length, data_max_length_seconds, folder, train_dataloader, val_dataloader, lr, grad_accum_every, wd, max_grad_norm, discr_max_grad_norm, save_results_every, save_model_every, log_losses_every, results_folder, valid_frac, random_split_seed, use_ema, ema_beta, ema_update_after_step, ema_update_every, apply_grad_penalty_every, dl_num_workers, accelerator, accelerate_kwargs, init_process_group_timeout_seconds, dataloader_drop_last, split_batches, use_wandb_tracking, force_clear_prev_results)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03mInitialize with a SoundStream instance and either a folder containing audio data or\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03mtrain/val DataLoader instances.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 200\u001b[0m \u001b[43mcheck_one_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m accelerator\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (exists(accelerator) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(accelerate_kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/audiolm_pytorch/trainer.py:64\u001b[0m, in \u001b[0;36mcheck_one_trainer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_one_trainer\u001b[39m():\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m ONE_TRAINER_INSTANTIATED\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ONE_TRAINER_INSTANTIATED, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monly one Trainer can be instantiated at a time for training\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     65\u001b[0m     ONE_TRAINER_INSTANTIATED \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","\u001b[0;31mAssertionError\u001b[0m: only one Trainer can be instantiated at a time for training"],"ename":"AssertionError","evalue":"only one Trainer can be instantiated at a time for training","output_type":"error"}]},{"cell_type":"markdown","source":"## 4. Training conditioning embeddings","metadata":{}},{"cell_type":"markdown","source":"Defining the MuLaN Embed Quantizer and Hubert K-means Embedder","metadata":{}},{"cell_type":"code","source":"quantizer = MuLaNEmbedQuantizer(\n    mulan=mulan,                         \n    **MULAN_QUANTIZER_KWARGS\n)\n\nwav2vec = HubertWithKmeans(\n    **HUBERT_KWARGS\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:24:19.833724Z","iopub.status.idle":"2023-11-23T03:24:19.835065Z","shell.execute_reply.started":"2023-11-23T03:24:19.834421Z","shell.execute_reply":"2023-11-23T03:24:19.834506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training Semantic Transformer","metadata":{}},{"cell_type":"code","source":"semantic_transformer = SemanticTransformer(\n   num_semantic_tokens=wav2vec.codebook_size,\n   **SEMANTIC_KWARGS \n).to(DEVICE)\n\nsemantic_trainer = SemanticTransformerTrainer(\n    wav2vec,\n    semantic_transformer,\n    audio_conditioner=quantizer,\n    **TRANSFORMER_TRAINER_KWARGS\n)\n\nsemantic_trainer.train()\n\ndel semantic_trainer","metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:24:19.838800Z","iopub.status.idle":"2023-11-23T03:24:19.839468Z","shell.execute_reply.started":"2023-11-23T03:24:19.839129Z","shell.execute_reply":"2023-11-23T03:24:19.839159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training Coarse Transformer","metadata":{}},{"cell_type":"code","source":"soundstream = MusicLMSoundStream()\n\nsoundstream.load(soundstream_ckpt)\n\ncoarse_transformer = CoarseTransformer(\n    num_semantic_tokens=wav2vec.codebook_size,\n    **COARSE_KWARGS\n).to(DEVICE)\n\ncoarse_trainer = CoarseTransformerTrainer(\n    wav2vec,\n    semantic_transformer,\n    codec=soundstream,\n    audio_conditioner=quantizer,\n    **TRANSFORMER_TRAINER_KWARGS\n)\n\ncoarse_trainer.train()\n\ndel coarse_trainer","metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:24:19.841735Z","iopub.status.idle":"2023-11-23T03:24:19.842565Z","shell.execute_reply.started":"2023-11-23T03:24:19.842025Z","shell.execute_reply":"2023-11-23T03:24:19.842054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training Fine Transformer","metadata":{}},{"cell_type":"code","source":"soundstream = MusicLMSoundStream()\n\nsoundstream.load(soundstream_ckpt)\n\nfine_transformer = FineTransformer(\n    codebook_size=wav2vec.codebook_size,\n    **FINE_KWARGS\n).to(DEVICE)\n\nfine_trainer = FineTransformerTrainer(\n    wav2vec,\n    semantic_transformer,\n    codec=soundstream\n    audio_conditioner=quantizer,\n    **TRANSFORMER_TRAINER_KWARGS\n)\n\nfine_trainer.train()\n\ndel fine_trainer","metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:24:19.845007Z","iopub.status.idle":"2023-11-23T03:24:19.845609Z","shell.execute_reply.started":"2023-11-23T03:24:19.845397Z","shell.execute_reply":"2023-11-23T03:24:19.845419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Combining AudioLM and MusicLM","metadata":{}},{"cell_type":"code","source":"audio_lm = AudioLM(\n    wav2vec=wav2vec,\n    codec=soundstream,\n    semantic_transformer=semantic_transformer,\n    coarse_transformer=coarse_transformer,\n    fine_transformer=fine_transformer   \n)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:24:19.848530Z","iopub.status.idle":"2023-11-23T03:24:19.848972Z","shell.execute_reply.started":"2023-11-23T03:24:19.848762Z","shell.execute_reply":"2023-11-23T03:24:19.848783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"music_lm = MusicLM(\n    audio_lm=audio_lm,\n    mulan_embed_quantizer=quantizer\n)\n\nmusic = music_lm('Café Au Lait 1970s', num_samples=3)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:24:19.851148Z","iopub.status.idle":"2023-11-23T03:24:19.852276Z","shell.execute_reply.started":"2023-11-23T03:24:19.852024Z","shell.execute_reply":"2023-11-23T03:24:19.852047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(music, 'generated_music.pt')","metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:24:19.854502Z","iopub.status.idle":"2023-11-23T03:24:19.855034Z","shell.execute_reply.started":"2023-11-23T03:24:19.854821Z","shell.execute_reply":"2023-11-23T03:24:19.854842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_path = \"out.wav\"\nsample_rate = 44100\ntorchaudio.save(output_path, music.cpu(), sample_rate)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:24:19.858070Z","iopub.status.idle":"2023-11-23T03:24:19.858682Z","shell.execute_reply.started":"2023-11-23T03:24:19.858317Z","shell.execute_reply":"2023-11-23T03:24:19.858338Z"},"trusted":true},"execution_count":null,"outputs":[]}]}