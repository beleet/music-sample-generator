# Music Sample Generation


## References

### Papers (2023)

| Title and demo page | Paper | Code |
| - | - | - |
| [Noise2Music: Text-conditioned Music Generation with Diffusion Models](https://google-research.github.io/noise2music/) | [arXiv](https://arxiv.org/abs/2302.03917) | |
| [MusicLM: Generating Music From Text paper](https://google-research.github.io/seanet/musiclm/examples/)| [arXiv](https://arxiv.org/abs/2301.11325)| [GitHub (unofficial)](https://github.com/lucidrains/musiclm-pytorch) |
| [MusicGen: Simple and Controllable Music Generation paper](https://ai.honu.io/papers/musicgen/)| [arXiv](https://arxiv.org/abs/2306.05284) | [Github](https://github.com/facebookresearch/audiocraft) |
| [Mo√ªsai: Text-to-Music Generation with Long-Context Latent Diffusion](https://anonymous0.notion.site/Mo-sai-Text-to-Audio-with-Long-Context-Latent-Diffusion-b43dbc71caf94b5898f9e8de714ab5dc)| [arXiv](https://arxiv.org/abs/2301.11757) | [GitHub](https://github.com/archinetai/audio-diffusion-pytorch) |
| [Msanii: High Fidelity Music Synthesis on a Shoestring Budget](https://kinyugo.github.io/msanii-demo/)| [arXiv](https://arxiv.org/abs/2301.06468) | [GitHub](https://github.com/Kinyugo/msanii) |
| [JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models](https://www.futureverse.com/research/jen/demos/jen1)| [arXiv](https://arxiv.org/abs/2308.04729)| |
| [MeLoDy: Efficient Neural Music Generation](https://efficient-melody.github.io/)| [arXiv](https://arxiv.org/abs/2305.15719) | |

- Thanks to [Audio AI Timeline](https://github.com/archinetai/audio-ai-timeline/blob/main/README.md)

### Repositories 

- [Musika](https://github.com/marcoppasini/musika)
- [Riffusion](https://github.com/riffusion/riffusion)
- [Multi-instrument Music Synthesis with Spectrogram Diffusion](https://github.com/magenta/music-spectrogram-diffusion)